{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57524e99-371b-4f35-8064-c553e5b50083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import laspy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def read_las(pointcloudfile, get_attributes=False, useevery=1):\n",
    "    \"\"\"\n",
    "    :param pointcloudfile: specification of input file (format: las or laz)\n",
    "    :param get_attributes: if True, will return all attributes in file, otherwise will only return XYZ (default is False)\n",
    "    :param useevery: value specifies every n-th point to use from input, i.e. simple subsampling (default is 1, i.e. returning every point)\n",
    "    :return: 3D array of points (x,y,z) of length number of points in input file (or subsampled by 'useevery')\n",
    "    \"\"\"\n",
    "\n",
    "    # Read file\n",
    "    inFile = laspy.read(pointcloudfile)\n",
    "\n",
    "    # Get coordinates (XYZ)\n",
    "    coords = np.vstack((inFile.x, inFile.y, inFile.z)).transpose()\n",
    "    coords = coords[::useevery, :]\n",
    "\n",
    "    # Return coordinates only\n",
    "    if get_attributes == False:\n",
    "        return coords\n",
    "\n",
    "    # Return coordinates and attributes\n",
    "    else:\n",
    "        las_fields = [info.name for info in inFile.points.point_format.dimensions]\n",
    "        attributes = {}\n",
    "        for las_field in las_fields[3:]:  # skip the X,Y,Z fields\n",
    "            attributes[las_field] = inFile.points[las_field][::useevery]\n",
    "        return (coords, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e22489-959e-4cb8-b991-9bb22c05a57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PointCloudsInFiles(InMemoryDataset):\n",
    "    \"\"\"Point cloud dataset where one data point is a file.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root_dir, glob=\"*\", column_name=\"\", max_points=200_000, use_columns=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with the datasets\n",
    "            glob (string): Glob string passed to pathlib.Path.glob\n",
    "            column_name (string): Column name to use as target variable (e.g. \"Classification\")\n",
    "            use_columns (list[string]): Column names to add as additional input\n",
    "        \"\"\"\n",
    "        self.files = list(Path(root_dir).glob(glob))\n",
    "        self.column_name = column_name\n",
    "        self.max_points = max_points\n",
    "        if use_columns is None:\n",
    "            use_columns = []\n",
    "        self.use_columns = use_columns\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        filename = str(self.files[idx])\n",
    "        coords, attrs = read_las(filename, get_attributes=True)\n",
    "        if coords.shape[0] >= self.max_points:\n",
    "            use_idx = np.random.choice(coords.shape[0], self.max_points, replace=False)\n",
    "        else:\n",
    "            use_idx = np.random.choice(coords.shape[0], self.max_points, replace=True)\n",
    "        if len(self.use_columns) > 0:\n",
    "            x = np.empty((self.max_points, len(self.use_columns)), np.float32)\n",
    "            for eix, entry in enumerate(self.use_columns):\n",
    "                x[:, eix] = attrs[entry][use_idx]\n",
    "        else:\n",
    "            x = coords[use_idx, :]\n",
    "        coords = coords - np.mean(coords, axis=0)  # centralize coordinates\n",
    "\n",
    "        # impute target\n",
    "        target = attrs[self.column_name]\n",
    "        target[np.isnan(target)] = np.nanmean(target)\n",
    "\n",
    "        sample = Data(\n",
    "            x=torch.from_numpy(x).float(),\n",
    "            y=torch.from_numpy(\n",
    "                np.unique(np.array(target[use_idx][:, np.newaxis]))\n",
    "            ).type(torch.LongTensor),\n",
    "            pos=torch.from_numpy(coords[use_idx, :]).float(),\n",
    "        )\n",
    "        if coords.shape[0] < 100:\n",
    "            return None\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde72adb-02dd-4d2c-99de-dcd6d281d871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import XConv, fps, global_mean_pool\n",
    "\n",
    "\n",
    "class PointCNN(nn.Module):\n",
    "    def __init__(self, numfeatures):\n",
    "        super().__init__()\n",
    "        self.numfeatures = numfeatures\n",
    "\n",
    "        # First XConv layer\n",
    "        self.conv1 = XConv(\n",
    "            self.numfeatures, 48, dim=3, kernel_size=8, hidden_channels=32\n",
    "        )\n",
    "\n",
    "        # Second XConv layer\n",
    "        self.conv2 = XConv(\n",
    "            48, 96, dim=3, kernel_size=12, hidden_channels=64, dilation=2\n",
    "        )\n",
    "\n",
    "        # Third XConv layer\n",
    "        self.conv3 = XConv(\n",
    "            96, 192, dim=3, kernel_size=16, hidden_channels=128, dilation=2\n",
    "        )\n",
    "\n",
    "        # Fourth XConv layer\n",
    "        self.conv4 = XConv(\n",
    "            192, 384, dim=3, kernel_size=16, hidden_channels=256, dilation=2\n",
    "        )\n",
    "\n",
    "        # Multilayer Perceptrons (MLPs) at the end of the PointCNN\n",
    "        self.lin1 = nn.Linear(384, 256)\n",
    "        self.lin2 = nn.Linear(256, 128)\n",
    "        self.lin3 = nn.Linear(128, 8)  # change last value for number of classes\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos, batch = data.pos, data.batch\n",
    "        x = data.x if self.numfeatures else None\n",
    "\n",
    "        # First XConv with no features\n",
    "        x = F.relu(self.conv1(x, pos, batch))\n",
    "        # x = torch.nn.ReLU(self.conv1(x, pos, batch))\n",
    "\n",
    "        # Farthest point sampling, keeping only 37.5%\n",
    "        idx = fps(pos, batch, ratio=0.375)\n",
    "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
    "        # Second XConv\n",
    "        x = F.relu(self.conv2(x, pos, batch))\n",
    "\n",
    "        # Farthest point samplling, keepiong only 33.4%\n",
    "        idx = fps(pos, batch, ratio=0.334)\n",
    "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
    "\n",
    "        # Two additional XConvs\n",
    "        x = F.relu(self.conv3(x, pos, batch))\n",
    "        x = F.relu(self.conv4(x, pos, batch))\n",
    "\n",
    "        # Pooling batch-elements together\n",
    "        # Each tree is described in one single point with 384 features\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # MLPs at the end with ReLU\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "\n",
    "        # Dropout: Set randomly to value of zero\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.dropout(x, p=0.5, training=True)\n",
    "        return self.lin3(x)\n",
    "\n",
    "        # # log-SofMax activation to callculate Negative Log Likelihood (NLL)\n",
    "        # return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512dc781-abe1-4923-a13d-42a8af615561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f5b17-27ca-46e1-8ea0-5dde41c341db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IOStream:\n",
    "    def __init__(self, path):\n",
    "        self.f = open(path, \"a\")\n",
    "\n",
    "    def cprint(self, text):\n",
    "        print(text)\n",
    "        self.f.write(text + \"\\n\")\n",
    "        self.f.flush\n",
    "\n",
    "    def close(self):\n",
    "        sefl.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4c1c6-7e45-4424-8780-4f209f56e1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _init_(model_name):\n",
    "    if not os.path.exists(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name):\n",
    "        os.makedirs(\"checkpoints/\" + model_name)\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/models\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/models\")\n",
    "\n",
    "\n",
    "def test_one_epoch(device, model, test_loader):\n",
    "    model.eval()  # https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
    "    test_loss = 0.0\n",
    "    pred = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for i, data in enumerate(tqdm(test_loader, desc=\"Testing\", leave=False)):\n",
    "        data.to(device)\n",
    "\n",
    "        # Call model\n",
    "        output = model(data)\n",
    "\n",
    "        # Define validation loss using negative log likelihood loss and softmax\n",
    "        loss_val = torch.nn.functional.nll_loss(\n",
    "            torch.nn.functional.log_softmax(output, dim=1),\n",
    "            target=data.y,\n",
    "            size_average=False,\n",
    "        )\n",
    "\n",
    "        # Update test_lost and count\n",
    "        test_loss += loss_val.item()\n",
    "        count += output.size(0)\n",
    "\n",
    "        # Update pred\n",
    "        _, pred1 = output.max(dim=1)\n",
    "        ag = pred1 == data.y\n",
    "        am = ag.sum()\n",
    "        pred += am.item()\n",
    "\n",
    "    # Calculate test_loss and accuracy\n",
    "    test_loss = float(test_loss) / count\n",
    "    accuracy = float(pred) / count\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def test(device, model, test_loader, textio):\n",
    "    test_loss, test_accuracy = test_one_epoch(device, model, test_loader)\n",
    "    textio.cprint(\n",
    "        \"Validation Loss: %f & Validation Accuracy: %f\" % (test_loss, test_accuracy)\n",
    "    )\n",
    "\n",
    "\n",
    "def train_one_epoch(device, model, train_loader, optimizer, epoch_number):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    pred = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for i, data in enumerate(\n",
    "        tqdm(train_loader, desc=\"Epoch: \" + str(epoch_number), leave=False)\n",
    "    ):\n",
    "        # Send data to device\n",
    "        data.to(device)\n",
    "\n",
    "        # Call model\n",
    "        output = model(data)\n",
    "\n",
    "        # Define validation loss using negative log likelihood loss and softmax\n",
    "        loss_val = torch.nn.functional.nll_loss(\n",
    "            torch.nn.functional.log_softmax(output, dim=1),\n",
    "            target=data.y,\n",
    "            size_average=False,\n",
    "        )\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update train_loss and count\n",
    "        train_loss += loss_val.item()\n",
    "        count += output.size(0)\n",
    "\n",
    "        # Update pred\n",
    "        _, pred1 = output.max(dim=1)\n",
    "        ag = pred1 == data.y\n",
    "        am = ag.sum()\n",
    "        pred += am.item()\n",
    "\n",
    "    # Calculate train_loss and accuracy\n",
    "    train_loss = float(train_loss) / count\n",
    "    accuracy = float(pred) / count\n",
    "\n",
    "    return train_loss, accuracy\n",
    "\n",
    "\n",
    "def train(\n",
    "    device,\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    boardio,\n",
    "    textio,\n",
    "    checkpoint,\n",
    "    model_name,\n",
    "    optimizer=\"Adam\",\n",
    "    start_epoch=0,\n",
    "    epochs=200,\n",
    "):\n",
    "    # Set up optimizer\n",
    "    learnable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if optimizer == \"Adam\":  # Adam optimizer\n",
    "        optimizer = torch.optim.Adam(learnable_params)\n",
    "    else:  # SGD optimizer\n",
    "        optimizer = torch.optim.SGD(learnable_params, lr=0.1)\n",
    "\n",
    "    # Set up checkpoint\n",
    "    if checkpoint is not None:\n",
    "        min_loss = checkpoint[\"min_loss\"]\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # Define best_test_loss\n",
    "    best_test_loss = np.inf\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # Train Model\n",
    "        train_loss, train_accuracy = train_one_epoch(\n",
    "            device, model, train_loader, optimizer, epoch + 1\n",
    "        )\n",
    "\n",
    "        # Test Model\n",
    "        test_loss, test_accuracy = test_one_epoch(device, model, test_loader)\n",
    "\n",
    "        # Save Best Model\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            snap = {\n",
    "                # state_dict: https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"min_loss\": best_test_loss,\n",
    "                \"optimizer\": optimizer.state_dict,\n",
    "            }\n",
    "            torch.save(snap, f\"checkpoints/{model_name}/models/best_model_snap.t7\")\n",
    "            torch.save(\n",
    "                model.state_dict, f\"checkpoints/{model_name}/models/best_model.t7\"\n",
    "            )\n",
    "\n",
    "        # Save model\n",
    "        torch.save(snap, f\"checkpoints/{model_name}/models/model_snap.t7\")\n",
    "        torch.save(model.state_dict, f\"checkpoints/{model_name}/models/model.t7\")\n",
    "\n",
    "        boardio.add_scalar(\"Train Loss\", train_loss, epoch + 1)\n",
    "        boardio.add_scalar(\"Test Loss\", test_loss, epoch + 1)\n",
    "        boardio.add_scalar(\"Best Test Loss\", best_test_loss, epoch + 1)\n",
    "        boardio.add_scalar(\"Train Accuracy\", train_accuracy, epoch + 1)\n",
    "        boardio.add_scalar(\"Test Accuracy\", test_accuracy, epoch + 1)\n",
    "\n",
    "        textio.cprint(\n",
    "            \"EPOCH:: %d, Training Loss: %f, Testing Loss: %f, Best Loss: %f\"\n",
    "            % (epoch + 1, train_loss, test_loss, best_test_loss)\n",
    "        )\n",
    "        textio.cprint(\n",
    "            \"EPOCH:: %d, Training Accuracy: %f Testing Accuracy: %f\"\n",
    "            % (epoch + 1, train_accuracy, test_accuracy)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702945fb-aa4b-4957-9e27-417dd00580b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_dataset_path = r\"D:\\MurrayBrent\\git\\point-dl\\input\\train\"\n",
    "    test_dataset_path = r\"D:\\MurrayBrent\\git\\point-dl\\input\\test\"\n",
    "    model_name = \"PointCNN\"\n",
    "    use_columns = [\"intensity\"]\n",
    "\n",
    "    boardio = SummaryWriter(log_dir=\"checkpoints/\" + model_name)\n",
    "    _init_(model_name)\n",
    "\n",
    "    textio = IOStream(\"checkpoints/\" + model_name + \"/run.log\")\n",
    "    textio.cprint(model_name)\n",
    "\n",
    "    # Get training and test datasets\n",
    "    trainset = PointCloudsInFiles(\n",
    "        train_dataset_path, \"*.laz\", \"Class\", max_points=1024, use_columns=use_columns\n",
    "    )\n",
    "    testset = PointCloudsInFiles(\n",
    "        test_dataset_path, \"*.laz\", \"Class\", max_points=1024, use_columns=use_columns\n",
    "    )\n",
    "\n",
    "    # Load training and test datasets\n",
    "    train_loader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(testset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Define device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = PointCNN(numfeatures=len(use_columns))\n",
    "\n",
    "    checkpoint = None\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    train(\n",
    "        device=device,\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        boardio=boardio,\n",
    "        textio=textio,\n",
    "        checkpoint=checkpoint,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91552c2-542a-4995-a3de-bed463829a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe42c3d-b449-43c0-b9ac-c900fea08381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch Python 3.9",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
