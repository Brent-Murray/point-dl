{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488dea7-0d4f-45dc-85d7-a55f94566150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import XConv, fps, global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5db28b7-8573-4548-9980-d58b7d5cb90f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T19:58:55.720237Z",
     "iopub.status.busy": "2022-06-03T19:58:55.719242Z",
     "iopub.status.idle": "2022-06-03T19:58:55.753091Z",
     "shell.execute_reply": "2022-06-03T19:58:55.752095Z",
     "shell.execute_reply.started": "2022-06-03T19:58:55.720237Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PointCNN(pl.LightningModule):\n",
    "    def __init__(self, numfeatures=1):\n",
    "        super().__init__()\n",
    "        self.learning_rate = 1e-3  # learning rate\n",
    "        self.train_acc = torchmetrics.Accuracy(\n",
    "            full_state_update=False\n",
    "        )  # traning accuracy\n",
    "        self.val_acc = torchmetrics.Accuracy(\n",
    "            full_state_update=False\n",
    "        )  # validation accuracy\n",
    "        self.test_acc = torchmetrics.Accuracy(full_state_update=False)  # test accuracy\n",
    "        self.numfeatures = numfeatures  # number of features\n",
    "\n",
    "        # First XConv layer\n",
    "        self.conv1 = XConv(\n",
    "            self.numfeatures, 48, dim=3, kernel_size=8, hidden_channels=32\n",
    "        )\n",
    "\n",
    "        # Second XConv layer\n",
    "        self.conv2 = XConv(\n",
    "            48, 96, dim=3, kernel_size=12, hidden_channels=64, dilation=2\n",
    "        )\n",
    "\n",
    "        # Third XConv layer\n",
    "        self.conv3 = XConv(\n",
    "            96, 192, dim=3, kernel_size=16, hidden_channels=128, dilation=2\n",
    "        )\n",
    "\n",
    "        # Fourth XConv layer\n",
    "        self.conv4 = XConv(\n",
    "            192, 384, dim=3, kernel_size=16, hidden_channels=256, dilation=2\n",
    "        )\n",
    "\n",
    "        # Multilayer Perceptrons (MLPs) at the end of the PointCNN\n",
    "        self.lin1 = nn.Linear(384, 256)\n",
    "        self.lin2 = nn.Linear(256, 128)\n",
    "        self.lin3 = nn.Linear(128, 8)  # change last value for number of classes\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos, batch = data.pos, data.batch\n",
    "        x = data.x if self.numfeatures else None\n",
    "\n",
    "        # First XConv with no features\n",
    "        x = F.relu(self.conv1(x, pos, batch))\n",
    "        # x = torch.nn.ReLU(self.conv1(x, pos, batch))\n",
    "\n",
    "        # Farthest point sampling, keeping only 37.5%\n",
    "        idx = fps(pos, batch, ratio=0.375)\n",
    "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
    "        # Second XConv\n",
    "        x = F.relu(self.conv2(x, pos, batch))\n",
    "\n",
    "        # Farthest point samplling, keepiong only 33.4%\n",
    "        idx = fps(pos, batch, ratio=0.334)\n",
    "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
    "\n",
    "        # Two additional XConvs\n",
    "        x = F.relu(self.conv3(x, pos, batch))\n",
    "        x = F.relu(self.conv4(x, pos, batch))\n",
    "\n",
    "        # Pooling batch-elements together\n",
    "        # Each tree is described in one single point with 384 features\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # MLPs at the end with ReLU\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "\n",
    "        # Dropout: Set randomly to value of zero\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.dropout(x, p=0.5, training=True)\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        # log-SofMax activation to callculate Negative Log Likelihood (NLL)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        y = data.y\n",
    "        out = self(data)\n",
    "        loss = F.nll_loss(out, y)\n",
    "        self.train_acc(out, y)\n",
    "        self.log(\n",
    "            \"train_acc\", self.train_acc, on_step=True, on_epoch=True, batch_size=16\n",
    "        )\n",
    "        self.log(\"train_loss\", loss, batch_size=16)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, data, batch_idx):\n",
    "        y = data.y\n",
    "        out = self(data)\n",
    "        val_loss = F.nll_loss(out, y)\n",
    "        self.val_acc(out, y)\n",
    "        self.log(\"val_acc\", self.val_acc, on_step=True, on_epoch=True, batch_size=16)\n",
    "        self.log(\"val_loss\", val_loss, batch_size=16)  # , on_step=True, on_epoch=True)\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, data, batch_idx):\n",
    "        y = data.y\n",
    "        out = self(data)\n",
    "        test_loss = F.nll_loss(out, y)\n",
    "        self.test_acc(out, y)\n",
    "        self.log(\"test_loss\", test_loss, batch_size=16)\n",
    "        # return out\n",
    "        return {\"test_loss\": test_loss, \"logits\": logits, \"labels\": y}\n",
    "\n",
    "    def test_step_end(self, outs):\n",
    "        return outs\n",
    "\n",
    "    def test_epoch_ends(self, outs):\n",
    "        global all_preds\n",
    "        globalall_labels\n",
    "        for out in outs:\n",
    "            probs = list(out[\"logits\"].cpu().detach().numpy())\n",
    "            labels = list(out[\"labels\"].flatten().cpu().detach().numpy())\n",
    "            all_preds.extend(probs)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # def test_epoch_end(self, outs):\n",
    "    #     global res\n",
    "    #     res = outs\n",
    "    #     return outs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch Python 3.9",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
