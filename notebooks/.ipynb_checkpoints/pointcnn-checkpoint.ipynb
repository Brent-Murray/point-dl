{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b66bcc-abee-4e33-9f7d-5b4c3fa9c5d7",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa805e-8054-4c62-a193-c6abcc2dca5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import laspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch_geometric.data import Data, InMemoryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b7b7b-b6c7-4edf-85ef-a1c2ad8eb09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_las(pointcloudfile, get_attributes=False, useevery=1):\n",
    "    \"\"\"\n",
    "    :param pointcloudfile: specification of input file (format: las or laz)\n",
    "    :param get_attributes: if True, will return all attributes in file, otherwise will only return XYZ (default is False)\n",
    "    :param useevery: value specifies every n-th point to use from input, i.e. simple subsampling (default is 1, i.e. returning every point)\n",
    "    :return: 3D array of points (x,y,z) of length number of points in input file (or subsampled by 'useevery')\n",
    "    \"\"\"\n",
    "\n",
    "    # Read file\n",
    "    inFile = laspy.read(pointcloudfile)\n",
    "\n",
    "    # Get coordinates (XYZ)\n",
    "    coords = np.vstack((inFile.x, inFile.y, inFile.z)).transpose()\n",
    "    coords = coords[::useevery, :]\n",
    "\n",
    "    # Return coordinates only\n",
    "    if get_attributes == False:\n",
    "        return coords\n",
    "\n",
    "    # Return coordinates and attributes\n",
    "    else:\n",
    "        las_fields = [info.name for info in inFile.points.point_format.dimensions]\n",
    "        attributes = {}\n",
    "        # for las_field in las_fields[3:]:  # skip the X,Y,Z fields\n",
    "        for las_field in las_fields:  # get all fields\n",
    "            attributes[las_field] = inFile.points[las_field][::useevery]\n",
    "        return (coords, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bfd52-bbea-4299-af90-fbb0148bf044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rotate_points(coords):\n",
    "    rotation = np.random.uniform(-180, 180)\n",
    "    # Convert rotation values to radians\n",
    "    rotation = np.radians(rotation)\n",
    "\n",
    "    # Rotate point cloud\n",
    "    rot_mat = np.array(\n",
    "        [\n",
    "            [np.cos(rotation), -np.sin(rotation), 0],\n",
    "            [np.sin(rotation), np.cos(rotation), 0],\n",
    "            [0, 0, 1],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    aug_coords = coords\n",
    "    aug_coords[:, :3] = np.matmul(aug_coords[:, :3], rot_mat)\n",
    "    return aug_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be65f56-5155-4457-9a00-cfadac957596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def point_removal(coords, x=None):\n",
    "    # Get list of ids\n",
    "    idx = list(range(np.shape(coords)[0]))\n",
    "    random.shuffle(idx)  # shuffle ids\n",
    "    idx = np.random.choice(\n",
    "        idx, random.randint(len(idx) - 50, len(idx)), replace=False\n",
    "    )  # pick points randomly removing 0 - 50 points\n",
    "\n",
    "    # Remove random values\n",
    "    aug_coords = coords[idx, :]  # remove coords\n",
    "    if x is None:  # remove x\n",
    "        aug_x = aug_coords\n",
    "    else:\n",
    "        aug_x = x[idx, :]\n",
    "\n",
    "    return aug_coords, aug_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a7a9-ce75-4363-ae43-74cb15f4cc25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_noise(coords, dim, x=None):\n",
    "    # Random standard deviation value\n",
    "    random_noise_sd = np.random.uniform(0.01, 0.025)\n",
    "\n",
    "    # Add/Subtract noise\n",
    "    if np.random.uniform(0, 1) >= 0.5:  # 50% chance to add\n",
    "        aug_coords = coords + np.random.normal(\n",
    "            0, random_noise_sd, size=(np.shape(coords)[0], 3)\n",
    "        )\n",
    "        if x is None:\n",
    "            aug_x = aug_coords\n",
    "        else:\n",
    "            aug_x = x + np.random.normal(0, random_noise_sd, size=(np.shape(x)))\n",
    "    else:  # 50% chance to subtract\n",
    "        aug_coords = coords - np.random.normal(\n",
    "            0, random_noise_sd, size=(np.shape(coords)[0], 3)\n",
    "        )\n",
    "        if x is None:\n",
    "            aug_x = aug_coords\n",
    "        else:\n",
    "            aug_x = x - np.random.normal(0, random_noise_sd, size=(np.shape(x)))\n",
    "\n",
    "    # Randomly choose between 0 and 50 augmented noise points\n",
    "    use_idx = np.random.choice(\n",
    "        aug_coords.shape[0], random.randint(0, 50), replace=False\n",
    "    )\n",
    "    aug_coords = aug_coords[use_idx, :]  # get random points\n",
    "    aug_coords = np.append(coords, aug_coords, axis=0)  # add points\n",
    "    aug_x = aug_x[use_idx, :]  # get random point values\n",
    "    aug_x = np.append(x, aug_x)  # add random point values\n",
    "\n",
    "    if dim == 1:\n",
    "        aug_x = aug_x[:, np.newaxis]\n",
    "\n",
    "    return aug_coords, aug_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7f84a-30a5-4c77-81f9-be5e01cd309c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PointCloudsInFiles(InMemoryDataset):\n",
    "    \"\"\"Point cloud dataset where one data point is a file.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root_dir, glob=\"*\", column_name=\"\", max_points=200_000, use_columns=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with the datasets\n",
    "            glob (string): Glob string passed to pathlib.Path.glob\n",
    "            column_name (string): Column name to use as target variable (e.g. \"Classification\")\n",
    "            use_columns (list[string]): Column names to add as additional input\n",
    "        \"\"\"\n",
    "        self.files = list(Path(root_dir).glob(glob))\n",
    "        self.column_name = column_name\n",
    "        self.max_points = max_points\n",
    "        if use_columns is None:\n",
    "            use_columns = []\n",
    "        self.use_columns = use_columns\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return length\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Get file name\n",
    "        filename = str(self.files[idx])\n",
    "\n",
    "        # Read las/laz file\n",
    "        coords, attrs = read_las(filename, get_attributes=True)\n",
    "\n",
    "        # Resample number of points to max_points\n",
    "        if coords.shape[0] >= self.max_points:\n",
    "            use_idx = np.random.choice(coords.shape[0], self.max_points, replace=False)\n",
    "        else:\n",
    "            use_idx = np.random.choice(coords.shape[0], self.max_points, replace=True)\n",
    "\n",
    "        # Get x values\n",
    "        if len(self.use_columns) > 0:\n",
    "            x = np.empty((self.max_points, len(self.use_columns)), np.float32)\n",
    "            for eix, entry in enumerate(self.use_columns):\n",
    "                x[:, eix] = attrs[entry][use_idx]\n",
    "        else:\n",
    "            x = coords[use_idx, :]\n",
    "\n",
    "        # Get coords\n",
    "        coords = coords - np.mean(coords, axis=0)  # centralize coordinates\n",
    "\n",
    "        # impute target\n",
    "        target = attrs[self.column_name]\n",
    "        target[np.isnan(target)] = np.nanmean(target)\n",
    "\n",
    "        # Transform data to tensor\n",
    "        sample = Data(\n",
    "            x=torch.from_numpy(x).float(),\n",
    "            y=torch.from_numpy(\n",
    "                np.unique(np.array(target[use_idx][:, np.newaxis]))\n",
    "            ).type(torch.LongTensor),\n",
    "            pos=torch.from_numpy(coords[use_idx, :]).float(),\n",
    "        )\n",
    "        if coords.shape[0] < 100:\n",
    "            return None\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811e755-f7ef-45a0-89c9-c3b54e3a7a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AugmentPointCloudsInFiles(InMemoryDataset):\n",
    "    \"\"\"Point cloud dataset where one data point is a file.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root_dir, glob=\"*\", column_name=\"\", max_points=200_000, use_columns=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with the datasets\n",
    "            glob (string): Glob string passed to pathlib.Path.glob\n",
    "            column_name (string): Column name to use as target variable (e.g. \"Classification\")\n",
    "            use_columns (list[string]): Column names to add as additional input\n",
    "        \"\"\"\n",
    "        self.files = list(Path(root_dir).glob(glob))\n",
    "        self.column_name = column_name\n",
    "        self.max_points = max_points\n",
    "        if use_columns is None:\n",
    "            use_columns = []\n",
    "        self.use_columns = use_columns\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return length\n",
    "        return len(self.files)  # NEED TO ADD MULTIPLICATION FOR NUMBER OF AUGMENTS\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Get file name\n",
    "        filename = str(self.files[idx])\n",
    "\n",
    "        # Read las/laz file\n",
    "        coords, attrs = read_las(filename, get_attributes=True)\n",
    "\n",
    "        # Resample number of points to max_points\n",
    "        if coords.shape[0] >= self.max_points:\n",
    "            use_idx = np.random.choice(coords.shape[0], self.max_points, replace=False)\n",
    "        else:\n",
    "            use_idx = np.random.choice(coords.shape[0], self.max_points, replace=True)\n",
    "\n",
    "        # Get x values\n",
    "        if len(self.use_columns) > 0:\n",
    "            x = np.empty((self.max_points, len(self.use_columns)), np.float32)\n",
    "            for eix, entry in enumerate(self.use_columns):\n",
    "                x[:, eix] = attrs[entry][use_idx]\n",
    "        else:\n",
    "            x = coords[use_idx, :]\n",
    "\n",
    "        # Get coords\n",
    "        coords = coords[use_idx, :]\n",
    "        coords = coords - np.mean(coords, axis=0)  # centralize coordinates\n",
    "\n",
    "        # Augmentation\n",
    "        coords, x = point_removal(coords, x)\n",
    "        coords, x = random_noise(coords, len(self.use_columns), x)\n",
    "        coords = rotate_points(coords)\n",
    "\n",
    "        # impute target\n",
    "        target = attrs[self.column_name]\n",
    "        target[np.isnan(target)] = np.nanmean(target)\n",
    "\n",
    "        # Transform data to tensor\n",
    "        sample = Data(\n",
    "            x=torch.from_numpy(x).float(),\n",
    "            y=torch.from_numpy(np.unique(np.array(target[:, np.newaxis]))).type(\n",
    "                torch.LongTensor\n",
    "            ),\n",
    "            pos=torch.from_numpy(coords).float(),\n",
    "        )\n",
    "        if coords.shape[0] < 100:\n",
    "            return None\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85374a-93f0-4a31-a1c0-4dfc282275a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IOStream:\n",
    "    def __init__(self, path):\n",
    "        # Open file in append\n",
    "        self.f = open(path, \"a\")\n",
    "\n",
    "    def cprint(self, text):\n",
    "        # Print and write text to file\n",
    "        print(text)  # print text\n",
    "        self.f.write(text + \"\\n\")  # write text and new line\n",
    "        self.f.flush  # flush file\n",
    "\n",
    "    def close(self):\n",
    "        sefl.f.close()  # close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2bbf35-dc8c-40af-8beb-67acc93280e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _init_(model_name):\n",
    "    # Create folder structure\n",
    "    if not os.path.exists(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name):\n",
    "        os.makedirs(\"checkpoints/\" + model_name)\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/models\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/models\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/confusion_matrix\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/confusion_matrix\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/confusion_matrix/all\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/confusion_matrix/all\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/confusion_matrix/best\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/confusion_matrix/best\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/classification_report\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/classification_report\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/classification_report/all\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/classification_report/all\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/classification_report/best\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/classification_report/best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1535b6-1034-478e-b36e-cc180b8be134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_confusion_matrix(\n",
    "    cf,\n",
    "    group_names=None,\n",
    "    categories=\"auto\",\n",
    "    count=True,\n",
    "    percent=True,\n",
    "    cbar=True,\n",
    "    xyticks=True,\n",
    "    xyplotlabels=True,\n",
    "    sum_stats=True,\n",
    "    figsize=None,\n",
    "    cmap=\"Blues\",\n",
    "    title=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "\n",
    "    percent:     If True, show the proportions for each category. Default is True.\n",
    "\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "\n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    \"\"\"\n",
    "\n",
    "    blanks = [\"\" for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names) == cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\n",
    "            \"{0:.2%}\".format(value) for value in cf.flatten() / np.sum(cf)\n",
    "        ]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [\n",
    "        f\"{v1}{v2}{v3}\".strip()\n",
    "        for v1, v2, v3 in zip(group_labels, group_counts, group_percentages)\n",
    "    ]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0], cf.shape[1])\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        # Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        # if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf) == 2:\n",
    "            # Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1, 1] / sum(cf[:, 1])\n",
    "            recall = cf[1, 1] / sum(cf[1, :])\n",
    "            f1_score = 2 * precision * recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy, precision, recall, f1_score\n",
    "            )\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize == None:\n",
    "        # Get default figure size if not set\n",
    "        figsize = plt.rcParams.get(\"figure.figsize\")\n",
    "\n",
    "    if xyticks == False:\n",
    "        # Do not show categories if xyticks is False\n",
    "        categories = False\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        cf,\n",
    "        annot=box_labels,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap,\n",
    "        cbar=cbar,\n",
    "        xticklabels=categories,\n",
    "        yticklabels=categories,\n",
    "    )\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\" + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f1d6b-d64e-4546-8fce-47f7d733bd8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_files(root_dir, glob=\"*\"):\n",
    "    # List files in root_dir with glob\n",
    "    files = list(Path(root_dir).glob(glob))\n",
    "\n",
    "    # Delete files\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc27757-e563-4d4e-9bfd-7ea5f88afa1b",
   "metadata": {},
   "source": [
    "# PointCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e98d6b-b1b5-4998-b34a-fc2b2309e4ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import XConv, fps, global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39fd57-d887-4332-abe0-fcf18cb68a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PointCNN(nn.Module):\n",
    "    def __init__(self, numfeatures, numclasses):\n",
    "        super().__init__()\n",
    "        self.numfeatures = numfeatures\n",
    "        self.numclasses = numclasses\n",
    "\n",
    "        # First XConv layer\n",
    "        self.conv1 = XConv(\n",
    "            self.numfeatures, 48, dim=3, kernel_size=8, hidden_channels=32\n",
    "        )\n",
    "\n",
    "        # Second XConv layer\n",
    "        self.conv2 = XConv(\n",
    "            48, 96, dim=3, kernel_size=12, hidden_channels=64, dilation=2\n",
    "        )\n",
    "\n",
    "        # Third XConv layer\n",
    "        self.conv3 = XConv(\n",
    "            96, 192, dim=3, kernel_size=16, hidden_channels=128, dilation=2\n",
    "        )\n",
    "\n",
    "        # Fourth XConv layer\n",
    "        self.conv4 = XConv(\n",
    "            192, 384, dim=3, kernel_size=16, hidden_channels=256, dilation=2\n",
    "        )\n",
    "\n",
    "        # Multilayer Perceptrons (MLPs) at the end of the PointCNN\n",
    "        self.lin1 = nn.Linear(384, 256)\n",
    "        self.lin2 = nn.Linear(256, 128)\n",
    "        self.lin3 = nn.Linear(\n",
    "            128, self.numclasses\n",
    "        )  # change last value for number of classes\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Get pos and batch\n",
    "        pos, batch = data.pos, data.batch\n",
    "\n",
    "        # Get x\n",
    "        x = data.x if self.numfeatures else None\n",
    "\n",
    "        # First XConv with no features\n",
    "        x = F.relu(self.conv1(x, pos, batch))\n",
    "        # x = torch.nn.ReLU(self.conv1(x, pos, batch))\n",
    "\n",
    "        # Farthest point sampling, keeping only 37.5%\n",
    "        idx = fps(pos, batch, ratio=0.375)\n",
    "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
    "        # Second XConv\n",
    "        x = F.relu(self.conv2(x, pos, batch))\n",
    "\n",
    "        # Farthest point samplling, keepiong only 33.4%\n",
    "        idx = fps(pos, batch, ratio=0.334)\n",
    "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
    "\n",
    "        # Two additional XConvs\n",
    "        x = F.relu(self.conv3(x, pos, batch))\n",
    "        x = F.relu(self.conv4(x, pos, batch))\n",
    "\n",
    "        # Pooling batch-elements together\n",
    "        # Each tree is described in one single point with 384 features\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # MLPs at the end with ReLU\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "\n",
    "        # Dropout: Set randomly to value of zero\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.dropout(x, p=0.5, training=True)\n",
    "        return self.lin3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc55f9b-9e56-44a6-9f67-ab0a898bff46",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25846f09-8727-45ee-bf88-2bb6496f88d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# from models.pointcnn import PointCNN\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from utils.tools import (\n",
    "#     IOStream,\n",
    "#     PointCloudsInFiles,\n",
    "#     _init_,\n",
    "#     delete_files,\n",
    "#     make_confusion_matrix,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df1d04-d22d-4435-bc48-dc0b96147a40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to datasets\n",
    "train_dataset_path = r\"D:\\MurrayBrent\\data\\RMF_ITD\\PLOT_LAS\\BUF_5M_SC\\train\"\n",
    "val_dataset_path = r\"D:\\MurrayBrent\\data\\RMF_ITD\\PLOT_LAS\\BUF_5M_SC\\val\"\n",
    "test_dataset_path = r\"D:\\MurrayBrent\\data\\RMF_ITD\\PLOT_LAS\\BUF_5M_SC\\test\"\n",
    "\n",
    "\n",
    "# Load pretrained model (\"\" if training)\n",
    "# pretrained = r\"D:\\MurrayBrent\\git\\point-dl\\notebooks\\checkpoints\\PointCNN_2048_6\\models\\best_model.t7\"\n",
    "pretrained = \"\"\n",
    "\n",
    "# max_points\n",
    "# 1024, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 9216, 10240,\n",
    "# 11264, 12288, 13312, 14336, 15360, 16384, 17408, 18432, 19456, 20480\n",
    "max_points = 2048\n",
    "\n",
    "# Fields to include from pointcloud\n",
    "use_columns = [\"intensity\"]\n",
    "\n",
    "# Classes: must be in same order as in data\n",
    "# classes = [\"Con\", \"Dec\"]\n",
    "classes = [\n",
    "    \"Jack Pine\",\n",
    "    \"White Spruce\",\n",
    "    \"Black Spruce\",\n",
    "    # \"Balsam Fir\",\n",
    "    # \"Eastern White Cedar\",\n",
    "    \"American Larch\",\n",
    "    \"Paper Birch\",\n",
    "    \"Trembling Aspen\",\n",
    "]\n",
    "\n",
    "# Model Name\n",
    "model_name = f\"PointCNN_{max_points}_{len(classes)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69d067-aa2f-4e51-8bce-13ba0574bdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_one_epoch(device, model, test_loader, testing=False):\n",
    "    model.eval()  # https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
    "    test_loss = 0.0\n",
    "    pred = 0.0\n",
    "    count = 0\n",
    "    y_pred = torch.tensor([], device=device)  # empty tensor\n",
    "    y_true = torch.tensor([], device=device)  # empty tensor\n",
    "    outs = torch.tensor([], device=device)  # empty tensor\n",
    "\n",
    "    # Iterate through data in loader\n",
    "    for i, data in enumerate(\n",
    "        tqdm(test_loader, desc=\"Validation\", leave=False, colour=\"green\")\n",
    "    ):\n",
    "        # Send data to defined device\n",
    "        data.to(device)\n",
    "\n",
    "        # Call model\n",
    "        output = model(data)\n",
    "\n",
    "        # Define validation loss using negative log likelihood loss and softmax\n",
    "        loss_val = torch.nn.functional.nll_loss(\n",
    "            torch.nn.functional.log_softmax(output, dim=1),\n",
    "            target=data.y,\n",
    "        )\n",
    "\n",
    "        # Update test_lost and count\n",
    "        test_loss += loss_val.item()\n",
    "        count += output.size(0)\n",
    "\n",
    "        # Update pred and true\n",
    "        _, pred1 = output.max(dim=1)\n",
    "        ag = pred1 == data.y\n",
    "        am = ag.sum()\n",
    "        pred += am.item()\n",
    "\n",
    "        y_true = torch.cat((y_true, data.y), 0)  # concatentate true values\n",
    "        y_pred = torch.cat((y_pred, pred1), 0)  # concatenate predicted values\n",
    "        outs = torch.cat((outs, output), 0)  # concatentate output\n",
    "\n",
    "    # Calculate test_loss and accuracy\n",
    "    test_loss = float(test_loss) / count\n",
    "    accuracy = float(pred) / count\n",
    "\n",
    "    # For validation\n",
    "    if testing is False:\n",
    "        # Create confusion matrix and classification report\n",
    "        y_true = y_true.cpu().numpy()  # convert to array and send to cpu\n",
    "        y_pred = y_pred.cpu().numpy()  # convert to array and send to cpu\n",
    "        conf_mat = confusion_matrix(y_true, y_pred)  # create confusion matrix\n",
    "        cls_rpt = classification_report(  # create classification report\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            target_names=classes,\n",
    "            labels=np.arange(len(classes)),\n",
    "            output_dict=True,\n",
    "            zero_division=1,\n",
    "        )\n",
    "        return test_loss, accuracy, conf_mat, cls_rpt\n",
    "\n",
    "    # For testing\n",
    "    else:\n",
    "        # out = torch.nn.functional.log_softmax(output, dim=1)  # softmax of output\n",
    "        out = torch.nn.functional.softmax(outs, dim=1)\n",
    "        return test_loss, accuracy, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53564232-0ce5-4205-b787-4434a87d6bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(device, model, test_loader, textio):\n",
    "    # Run test_one_epoch with testing as true\n",
    "    test_loss, test_accuracy, out = test_one_epoch(\n",
    "        device, model, test_loader, testing=True\n",
    "    )\n",
    "\n",
    "    # Print and save loss and accuracy\n",
    "    textio.cprint(\n",
    "        \"Testing Loss: %f & Testing Accuracy: %f\" % (test_loss, test_accuracy)\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af583747-af0c-467f-957b-fa7e703f9084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(device, model, train_loader, optimizer, epoch_number):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    pred = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through data in loader\n",
    "    for i, data in enumerate(\n",
    "        tqdm(\n",
    "            train_loader, desc=\"Epoch: \" + str(epoch_number), leave=False, colour=\"blue\"\n",
    "        )\n",
    "    ):\n",
    "        # Send data to device\n",
    "        data.to(device)\n",
    "\n",
    "        # Call model\n",
    "        output = model(data)\n",
    "\n",
    "        # Define validation loss using negative log likelihood loss and softmax\n",
    "        loss_val = torch.nn.functional.nll_loss(\n",
    "            torch.nn.functional.log_softmax(output, dim=1),\n",
    "            target=data.y,\n",
    "        )\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update train_loss and count\n",
    "        train_loss += loss_val.item()\n",
    "        count += output.size(0)\n",
    "\n",
    "        # Update pred\n",
    "        _, pred1 = output.max(dim=1)\n",
    "        ag = pred1 == data.y\n",
    "        am = ag.sum()\n",
    "        pred += am.item()\n",
    "\n",
    "    # Calculate train_loss and accuracy\n",
    "    train_loss = float(train_loss) / count\n",
    "    accuracy = float(pred) / count\n",
    "\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f45e2-dddf-4169-aa9c-55dfbc7c2459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    device,\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    boardio,\n",
    "    textio,\n",
    "    checkpoint,\n",
    "    model_name,\n",
    "    optimizer=\"Adam\",\n",
    "    start_epoch=0,\n",
    "    epochs=200,\n",
    "):\n",
    "    # Set up optimizer\n",
    "    learnable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if optimizer == \"Adam\":  # Adam optimizer\n",
    "        optimizer = torch.optim.Adam(learnable_params)\n",
    "    else:  # SGD optimizer\n",
    "        optimizer = torch.optim.SGD(learnable_params, lr=0.1)\n",
    "\n",
    "    # Set up checkpoint\n",
    "    if checkpoint is not None:\n",
    "        min_loss = checkpoint[\"min_loss\"]\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # Define best_test_loss\n",
    "    best_test_loss = np.inf\n",
    "\n",
    "    # Run for every epoch\n",
    "    for epoch in tqdm(\n",
    "        range(start_epoch, epochs), desc=\"Total\", leave=False, colour=\"red\"\n",
    "    ):\n",
    "        # Train Model\n",
    "        train_loss, train_accuracy = train_one_epoch(\n",
    "            device, model, train_loader, optimizer, epoch + 1\n",
    "        )\n",
    "\n",
    "        # Validate model: testing=False\n",
    "        test_loss, test_accuracy, conf_mat, cls_rpt = test_one_epoch(\n",
    "            device, model, test_loader, testing=False\n",
    "        )\n",
    "\n",
    "        # Save Best Model\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "\n",
    "            # Create snap dictionary\n",
    "            snap = {\n",
    "                # state_dict: https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"min_loss\": best_test_loss,\n",
    "                \"optimizer\": optimizer.state_dict,\n",
    "            }\n",
    "            # Save best snap dictionary\n",
    "            torch.save(snap, f\"checkpoints/{model_name}/models/best_model_snap.t7\")\n",
    "\n",
    "            # Save best model\n",
    "            torch.save(\n",
    "                model.state_dict(), f\"checkpoints/{model_name}/models/best_model.t7\"\n",
    "            )\n",
    "\n",
    "            # Make confusion matrix figure\n",
    "            make_confusion_matrix(conf_mat, categories=classes)\n",
    "\n",
    "            # Save best model confusion matrix\n",
    "            delete_files(\n",
    "                f\"checkpoints/{model_name}/confusion_matrix/best\", \"*.png\"\n",
    "            )  # delete previous\n",
    "\n",
    "            plt.savefig(\n",
    "                f\"checkpoints/{model_name}/confusion_matrix/best/confusion_matrix_epoch{epoch+1}.png\",\n",
    "                bbox_inches=\"tight\",\n",
    "                dpi=300,\n",
    "            )  # save png\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "            # Create classification report figure\n",
    "            sns.heatmap(pd.DataFrame(cls_rpt).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "\n",
    "            # save best model classification report\n",
    "            delete_files(\n",
    "                f\"checkpoints/{model_name}/classification_report/best\", \"*.png\"\n",
    "            )  # delete previous\n",
    "\n",
    "            plt.savefig(\n",
    "                f\"checkpoints/{model_name}/classification_report/best/classification_report_epoch{epoch+1}.png\",\n",
    "                bbox_inches=\"tight\",\n",
    "                dpi=300,\n",
    "            )  # save png\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "        # Create confusion matrix figure\n",
    "        make_confusion_matrix(conf_mat, categories=classes)\n",
    "        plt.savefig(\n",
    "            f\"checkpoints/{model_name}/confusion_matrix/all/confusion_matrix_epoch{epoch+1}.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=300,\n",
    "        )  # save .png\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        # Create classification report figure\n",
    "        sns.heatmap(pd.DataFrame(cls_rpt).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "        plt.savefig(\n",
    "            f\"checkpoints/{model_name}/classification_report/all/classification_report_epoch{epoch+1}.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=300,\n",
    "        )  # save .png\n",
    "        plt.close()\n",
    "\n",
    "        # Save most recent model\n",
    "        torch.save(snap, f\"checkpoints/{model_name}/models/model_snap.t7\")\n",
    "        torch.save(model.state_dict(), f\"checkpoints/{model_name}/models/model.t7\")\n",
    "\n",
    "        # Add values to TensorBoard\n",
    "        boardio.add_scalar(\"Train Loss\", train_loss, epoch + 1)  # training loss\n",
    "        boardio.add_scalar(\"Validation Loss\", test_loss, epoch + 1)  # validation loss\n",
    "        boardio.add_scalar(\n",
    "            \"Best Validation Loss\", best_test_loss, epoch + 1\n",
    "        )  # best validation loss\n",
    "        boardio.add_scalar(\n",
    "            \"Train Accuracy\", train_accuracy, epoch + 1\n",
    "        )  # training accuracy\n",
    "        boardio.add_scalar(\n",
    "            \"Validation Accuracy\", test_accuracy, epoch + 1\n",
    "        )  # validation accuracy\n",
    "        boardio.add_scalars(\n",
    "            \"Loss\",\n",
    "            {\"Training Loss\": train_loss, \"Validation Loss\": test_loss},\n",
    "            epoch + 1,\n",
    "        )  # training and validation loss\n",
    "\n",
    "        # Print and save losses and accuracies\n",
    "        textio.cprint(\n",
    "            \"EPOCH:: %d, Training Loss: %f, Validation Loss: %f, Best Loss: %f\"\n",
    "            % (epoch + 1, train_loss, test_loss, best_test_loss)\n",
    "        )\n",
    "        textio.cprint(\n",
    "            \"EPOCH:: %d, Training Accuracy: %f Validation Accuracy: %f\"\n",
    "            % (epoch + 1, train_accuracy, test_accuracy)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c981851-d15b-44a5-9136-0916db53653d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(pretrained=\"\", augment=True):\n",
    "    # Set up TensorBoard summary writer\n",
    "    boardio = SummaryWriter(log_dir=\"checkpoints/\" + model_name)\n",
    "    _init_(model_name)\n",
    "\n",
    "    # Set up logger\n",
    "    textio = IOStream(\"checkpoints/\" + model_name + \"/run.log\")\n",
    "    textio.cprint(model_name)\n",
    "\n",
    "    # Get training, validation and test datasets\n",
    "    if train_dataset_path:\n",
    "        trainset = PointCloudsInFiles(\n",
    "            train_dataset_path,\n",
    "            \"*.laz\",\n",
    "            \"Class\",\n",
    "            max_points=max_points,\n",
    "            use_columns=use_columns,\n",
    "        )\n",
    "\n",
    "        # Augment training data\n",
    "        if augment is True:\n",
    "            aug_trainset = AugmentPointCloudsInFiles(\n",
    "                train_dataset_path,\n",
    "                \"*.laz\",\n",
    "                \"Class\",\n",
    "                max_points=max_points,\n",
    "                use_columns=use_columns,\n",
    "            )\n",
    "\n",
    "            # Concat training and augmented training datasets\n",
    "            trainset = torch.utils.data.ConcatDataset([trainset, aug_trainset])\n",
    "        # Load training dataset\n",
    "        train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "    if val_dataset_path:\n",
    "        valset = PointCloudsInFiles(\n",
    "            val_dataset_path,\n",
    "            \"*.laz\",\n",
    "            \"Class\",\n",
    "            max_points=max_points,\n",
    "            use_columns=use_columns,\n",
    "        )\n",
    "        # Load validation dataset\n",
    "        val_loader = DataLoader(valset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "    if test_dataset_path:\n",
    "        testset = PointCloudsInFiles(\n",
    "            test_dataset_path,\n",
    "            \"*.laz\",\n",
    "            \"Class\",\n",
    "            max_points=max_points,\n",
    "            use_columns=use_columns,\n",
    "        )\n",
    "        # Load testing dataset\n",
    "        test_loader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Define device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define model\n",
    "    model = PointCNN(numfeatures=len(use_columns), numclasses=len(classes))\n",
    "\n",
    "    # Checkpoint\n",
    "    checkpoint = None\n",
    "\n",
    "    # Load existing model\n",
    "    if pretrained:\n",
    "        assert os.path.isfile(pretrained)\n",
    "        model.load_state_dict(torch.load(pretrained, map_location=\"cpu\"))\n",
    "\n",
    "    # Send model to defined device\n",
    "    model.to(device)\n",
    "\n",
    "    # Run testing\n",
    "    if pretrained:\n",
    "        finished = test(\n",
    "            device=device, model=model, test_loader=test_loader, textio=textio\n",
    "        )\n",
    "        return finished\n",
    "    # Run training\n",
    "    else:\n",
    "        train(\n",
    "            device=device,\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=val_loader,\n",
    "            boardio=boardio,\n",
    "            textio=textio,\n",
    "            checkpoint=checkpoint,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857bad0-e3d8-4802-875f-f4092ebf25e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Runtime\n",
    "if __name__ == \"__main__\":\n",
    "    main(pretrained=pretrained)\n",
    "    # value = main(pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71666de9-55b6-4a61-a909-25ee6f46c4b0",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01720aaa-77aa-4f12-baaf-950a014d0ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value = value.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66cd4c-f1b5-4cb8-9d92-19fe41b9d40e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "range(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e1d01-34f8-4c89-a3f0-ba963e4d7735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = list(Path(r\"D:\\MurrayBrent\\data\\RMF_ITD\\PLOT_LAS\\BUF_5M_SC\\test\").glob(\"*.laz\"))\n",
    "for i in range(len(value)):\n",
    "    print(files[i])\n",
    "    print(value[i])\n",
    "    print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch Python 3.9",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
