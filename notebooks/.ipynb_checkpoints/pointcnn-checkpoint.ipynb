{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57524e99-371b-4f35-8064-c553e5b50083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import laspy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def read_las(pointcloudfile, get_attributes=False, useevery=1):\n",
    "    \"\"\"\n",
    "    :param pointcloudfile: specification of input file (format: las or laz)\n",
    "    :param get_attributes: if True, will return all attributes in file, otherwise will only return XYZ (default is False)\n",
    "    :param useevery: value specifies every n-th point to use from input, i.e. simple subsampling (default is 1, i.e. returning every point)\n",
    "    :return: 3D array of points (x,y,z) of length number of points in input file (or subsampled by 'useevery')\n",
    "    \"\"\"\n",
    "\n",
    "    # Read file\n",
    "    inFile = laspy.read(pointcloudfile)\n",
    "\n",
    "    # Get coordinates (XYZ)\n",
    "    coords = np.vstack((inFile.x, inFile.y, inFile.z)).transpose()\n",
    "    coords = coords[::useevery, :]\n",
    "\n",
    "    # Return coordinates only\n",
    "    if get_attributes == False:\n",
    "        return coords\n",
    "\n",
    "    # Return coordinates and attributes\n",
    "    else:\n",
    "        las_fields = [info.name for info in inFile.points.point_format.dimensions]\n",
    "        attributes = {}\n",
    "        for las_field in las_fields[3:]:  # skip the X,Y,Z fields\n",
    "            attributes[las_field] = inFile.points[las_field][::useevery]\n",
    "        return (coords, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e22489-959e-4cb8-b991-9bb22c05a57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PointCloudsInFiles(InMemoryDataset):\n",
    "    \"\"\"Point cloud dataset where one data point is a file.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root_dir, glob=\"*\", column_name=\"\", max_points=200_000, use_columns=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with the datasets\n",
    "            glob (string): Glob string passed to pathlib.Path.glob\n",
    "            column_name (string): Column name to use as target variable (e.g. \"Classification\")\n",
    "            use_columns (list[string]): Column names to add as additional input\n",
    "        \"\"\"\n",
    "        self.files = list(Path(root_dir).glob(glob))\n",
    "        self.column_name = column_name\n",
    "        self.max_points = max_points\n",
    "        if use_columns is None:\n",
    "            use_columns = []\n",
    "        self.use_columns = use_columns\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        filename = str(self.files[idx])\n",
    "        coords, attrs = read_las(filename, get_attributes=True)\n",
    "        if coords.shape[0] >= self.max_points:\n",
    "            use_idx = np.random.choice(coords.shape[0], self.max_points, replace=False)\n",
    "        else:\n",
    "            use_idx = np.random.choice(coords.shape[0], self.max_points, replace=True)\n",
    "        if len(self.use_columns) > 0:\n",
    "            x = np.empty((self.max_points, len(self.use_columns)), np.float32)\n",
    "            for eix, entry in enumerate(self.use_columns):\n",
    "                x[:, eix] = attrs[entry][use_idx]\n",
    "        else:\n",
    "            x = coords[use_idx, :]\n",
    "        coords = coords - np.mean(coords, axis=0)  # centralize coordinates\n",
    "\n",
    "        # impute target\n",
    "        target = attrs[self.column_name]\n",
    "        target[np.isnan(target)] = np.nanmean(target)\n",
    "\n",
    "        sample = Data(\n",
    "            x=torch.from_numpy(x).float(),\n",
    "            y=torch.from_numpy(\n",
    "                np.unique(np.array(target[use_idx][:, np.newaxis]))\n",
    "            ).type(torch.LongTensor),\n",
    "            pos=torch.from_numpy(coords[use_idx, :]).float(),\n",
    "        )\n",
    "        if coords.shape[0] < 100:\n",
    "            return None\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde72adb-02dd-4d2c-99de-dcd6d281d871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import XConv, fps, global_mean_pool\n",
    "\n",
    "\n",
    "class PointCNN(nn.Module):\n",
    "    def __init__(self, numfeatures, numclasses):\n",
    "        super().__init__()\n",
    "        self.numfeatures = numfeatures\n",
    "        self.numclasses = numclasses\n",
    "\n",
    "        # First XConv layer\n",
    "        self.conv1 = XConv(\n",
    "            self.numfeatures, 48, dim=3, kernel_size=8, hidden_channels=32\n",
    "        )\n",
    "\n",
    "        # Second XConv layer\n",
    "        self.conv2 = XConv(\n",
    "            48, 96, dim=3, kernel_size=12, hidden_channels=64, dilation=2\n",
    "        )\n",
    "\n",
    "        # Third XConv layer\n",
    "        self.conv3 = XConv(\n",
    "            96, 192, dim=3, kernel_size=16, hidden_channels=128, dilation=2\n",
    "        )\n",
    "\n",
    "        # Fourth XConv layer\n",
    "        self.conv4 = XConv(\n",
    "            192, 384, dim=3, kernel_size=16, hidden_channels=256, dilation=2\n",
    "        )\n",
    "\n",
    "        # Multilayer Perceptrons (MLPs) at the end of the PointCNN\n",
    "        self.lin1 = nn.Linear(384, 256)\n",
    "        self.lin2 = nn.Linear(256, 128)\n",
    "        self.lin3 = nn.Linear(\n",
    "            128, self.numclasses\n",
    "        )  # change last value for number of classes\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos, batch = data.pos, data.batch\n",
    "        x = data.x if self.numfeatures else None\n",
    "\n",
    "        # First XConv with no features\n",
    "        x = F.relu(self.conv1(x, pos, batch))\n",
    "        # x = torch.nn.ReLU(self.conv1(x, pos, batch))\n",
    "\n",
    "        # Farthest point sampling, keeping only 37.5%\n",
    "        idx = fps(pos, batch, ratio=0.375)\n",
    "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
    "        # Second XConv\n",
    "        x = F.relu(self.conv2(x, pos, batch))\n",
    "\n",
    "        # Farthest point samplling, keepiong only 33.4%\n",
    "        idx = fps(pos, batch, ratio=0.334)\n",
    "        x, pos, batch = x[idx], pos[idx], batch[idx]\n",
    "\n",
    "        # Two additional XConvs\n",
    "        x = F.relu(self.conv3(x, pos, batch))\n",
    "        x = F.relu(self.conv4(x, pos, batch))\n",
    "\n",
    "        # Pooling batch-elements together\n",
    "        # Each tree is described in one single point with 384 features\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # MLPs at the end with ReLU\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "\n",
    "        # Dropout: Set randomly to value of zero\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.dropout(x, p=0.5, training=True)\n",
    "        return self.lin3(x)\n",
    "\n",
    "        # # log-SofMax activation to callculate Negative Log Likelihood (NLL)\n",
    "        # return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512dc781-abe1-4923-a13d-42a8af615561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T20:04:40.791252Z",
     "iopub.status.busy": "2022-06-09T20:04:40.790256Z",
     "iopub.status.idle": "2022-06-09T20:04:44.514505Z",
     "shell.execute_reply": "2022-06-09T20:04:44.513551Z",
     "shell.execute_reply.started": "2022-06-09T20:04:40.791252Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f5b17-27ca-46e1-8ea0-5dde41c341db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IOStream:\n",
    "    def __init__(self, path):\n",
    "        self.f = open(path, \"a\")\n",
    "\n",
    "    def cprint(self, text):\n",
    "        print(text)\n",
    "        self.f.write(text + \"\\n\")\n",
    "        self.f.flush\n",
    "\n",
    "    def close(self):\n",
    "        sefl.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73f945-a7c0-4ae1-9954-31f2a0c07757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def make_confusion_matrix(\n",
    "    cf,\n",
    "    group_names=None,\n",
    "    categories=\"auto\",\n",
    "    count=True,\n",
    "    percent=True,\n",
    "    cbar=True,\n",
    "    xyticks=True,\n",
    "    xyplotlabels=True,\n",
    "    sum_stats=True,\n",
    "    figsize=None,\n",
    "    cmap=\"Blues\",\n",
    "    title=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "\n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    \"\"\"\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = [\"\" for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names) == cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\n",
    "            \"{0:.2%}\".format(value) for value in cf.flatten() / np.sum(cf)\n",
    "        ]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [\n",
    "        f\"{v1}{v2}{v3}\".strip()\n",
    "        for v1, v2, v3 in zip(group_labels, group_counts, group_percentages)\n",
    "    ]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0], cf.shape[1])\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        # Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        # if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf) == 2:\n",
    "            # Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1, 1] / sum(cf[:, 1])\n",
    "            recall = cf[1, 1] / sum(cf[1, :])\n",
    "            f1_score = 2 * precision * recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy, precision, recall, f1_score\n",
    "            )\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize == None:\n",
    "        # Get default figure size if not set\n",
    "        figsize = plt.rcParams.get(\"figure.figsize\")\n",
    "\n",
    "    if xyticks == False:\n",
    "        # Do not show categories if xyticks is False\n",
    "        categories = False\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        cf,\n",
    "        annot=box_labels,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap,\n",
    "        cbar=cbar,\n",
    "        xticklabels=categories,\n",
    "        yticklabels=categories,\n",
    "    )\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.xlabel(\"Predicted label\" + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59094746-2160-466c-ba95-6532917e65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_report(cr, title=None, with_avg_total=False, cmap=\"Blues\"):\n",
    "\n",
    "    lines = cr.split(\"\\n\")\n",
    "\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    for line in lines[2 : (len(lines) - 3)]:\n",
    "        # print(line)\n",
    "        t = line.split()\n",
    "        # print(t)\n",
    "        classes.append(t[0])\n",
    "        # v = [float(x) for x in t[1 : len(t) - 1]]\n",
    "        v = [x for x in t[1 : len(t) - 1]]\n",
    "        print(v)\n",
    "        plotMat.append(v)\n",
    "\n",
    "    if with_avg_total:\n",
    "        aveTotal = lines[len(lines) - 1].split()\n",
    "        classes.append(\"avg/total\")\n",
    "        vAveTotal = [float(x) for x in t[1 : len(aveTotal) - 1]]\n",
    "        plotMat.append(vAveTotal)\n",
    "\n",
    "    plt.imshow(plotMat, interpolation=\"nearest\", cmap=cmap)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.colorbar()\n",
    "    x_tick_marks = np.arange(3)\n",
    "    y_tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(x_tick_marks, [\"precision\", \"recall\", \"f1-score\"], rotation=45)\n",
    "    plt.yticks(y_tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"Classes\")\n",
    "    plt.xlabel(\"Measures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4c1c6-7e45-4424-8780-4f209f56e1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _init_(model_name):\n",
    "    if not os.path.exists(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name):\n",
    "        os.makedirs(\"checkpoints/\" + model_name)\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/models\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/models\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/confusion_matrix\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/confusion_matrix\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/confusion_matrix/all\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/confusion_matrix/all\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/confusion_matrix/best\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/confusion_matrix/best\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/classification_report\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/classification_report\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/classification_report/all\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/classification_report/all\")\n",
    "    if not os.path.exists(\"checkpoints/\" + model_name + \"/classification_report/best\"):\n",
    "        os.makedirs(\"checkpoints/\" + model_name + \"/classification_report/best\")\n",
    "\n",
    "\n",
    "train_dataset_path = r\"D:\\MurrayBrent\\data\\RMF_ITD\\PLOT_LAS\\BUF_5M_TC\\train\"\n",
    "test_dataset_path = r\"D:\\MurrayBrent\\data\\RMF_ITD\\PLOT_LAS\\BUF_5M_TC\\test\"\n",
    "max_points = 2048\n",
    "model_name = f\"PointCNN_{max_points}\"\n",
    "use_columns = [\"intensity\"]\n",
    "classes = [\"Con\", \"Dec\"]\n",
    "\n",
    "\n",
    "def test_one_epoch(device, model, test_loader):\n",
    "    model.eval()  # https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
    "    test_loss = 0.0\n",
    "    pred = 0.0\n",
    "    count = 0\n",
    "    y_pred = torch.tensor([], device=device)\n",
    "    y_true = torch.tensor([], device=device)\n",
    "\n",
    "    for i, data in enumerate(\n",
    "        tqdm(test_loader, desc=\"Testing\", leave=False, colour=\"green\")\n",
    "    ):\n",
    "        data.to(device)\n",
    "\n",
    "        # Call model\n",
    "        output = model(data)\n",
    "\n",
    "        # Define validation loss using negative log likelihood loss and softmax\n",
    "        loss_val = torch.nn.functional.nll_loss(\n",
    "            torch.nn.functional.log_softmax(output, dim=1),\n",
    "            target=data.y,\n",
    "        )\n",
    "\n",
    "        # Update test_lost and count\n",
    "        test_loss += loss_val.item()\n",
    "        count += output.size(0)\n",
    "\n",
    "        # Update pred\n",
    "        _, pred1 = output.max(dim=1)\n",
    "        ag = pred1 == data.y\n",
    "        am = ag.sum()\n",
    "        pred += am.item()\n",
    "\n",
    "        y_true = torch.cat((y_true, data.y), 0)\n",
    "        y_pred = torch.cat((y_pred, pred1), 0)\n",
    "\n",
    "    # Calculate test_loss and accuracy\n",
    "    test_loss = float(test_loss) / count\n",
    "    accuracy = float(pred) / count\n",
    "\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    cls_rpt = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=classes,\n",
    "        labels=np.arange(len(classes)),\n",
    "        output_dict=True,\n",
    "        zero_division=1,\n",
    "    )\n",
    "\n",
    "    return test_loss, accuracy, conf_mat, cls_rpt\n",
    "\n",
    "\n",
    "def test(device, model, test_loader, textio):\n",
    "    test_loss, test_accuracy = test_one_epoch(device, model, test_loader)\n",
    "    textio.cprint(\n",
    "        \"Validation Loss: %f & Validation Accuracy: %f\" % (test_loss, test_accuracy)\n",
    "    )\n",
    "\n",
    "\n",
    "def train_one_epoch(device, model, train_loader, optimizer, epoch_number):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    pred = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for i, data in enumerate(\n",
    "        tqdm(\n",
    "            train_loader, desc=\"Epoch: \" + str(epoch_number), leave=False, colour=\"blue\"\n",
    "        )\n",
    "    ):\n",
    "        # Send data to device\n",
    "        data.to(device)\n",
    "\n",
    "        # Call model\n",
    "        output = model(data)\n",
    "\n",
    "        # Define validation loss using negative log likelihood loss and softmax\n",
    "        loss_val = torch.nn.functional.nll_loss(\n",
    "            torch.nn.functional.log_softmax(output, dim=1),\n",
    "            target=data.y,\n",
    "        )\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update train_loss and count\n",
    "        train_loss += loss_val.item()\n",
    "        count += output.size(0)\n",
    "\n",
    "        # Update pred\n",
    "        _, pred1 = output.max(dim=1)\n",
    "        ag = pred1 == data.y\n",
    "        am = ag.sum()\n",
    "        pred += am.item()\n",
    "\n",
    "    # Calculate train_loss and accuracy\n",
    "    train_loss = float(train_loss) / count\n",
    "    accuracy = float(pred) / count\n",
    "\n",
    "    return train_loss, accuracy\n",
    "\n",
    "\n",
    "def train(\n",
    "    device,\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    boardio,\n",
    "    textio,\n",
    "    checkpoint,\n",
    "    model_name,\n",
    "    optimizer=\"Adam\",\n",
    "    start_epoch=0,\n",
    "    epochs=200,\n",
    "):\n",
    "    # Set up optimizer\n",
    "    learnable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if optimizer == \"Adam\":  # Adam optimizer\n",
    "        optimizer = torch.optim.Adam(learnable_params)\n",
    "    else:  # SGD optimizer\n",
    "        optimizer = torch.optim.SGD(learnable_params, lr=0.1)\n",
    "\n",
    "    # Set up checkpoint\n",
    "    if checkpoint is not None:\n",
    "        min_loss = checkpoint[\"min_loss\"]\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # Define best_test_loss\n",
    "    best_test_loss = np.inf\n",
    "\n",
    "    for epoch in tqdm(\n",
    "        range(start_epoch, epochs), desc=\"Total\", leave=False, colour=\"red\"\n",
    "    ):\n",
    "        # Train Model\n",
    "        train_loss, train_accuracy = train_one_epoch(\n",
    "            device, model, train_loader, optimizer, epoch + 1\n",
    "        )\n",
    "\n",
    "        # Test Model\n",
    "        test_loss, test_accuracy, conf_mat, cls_rpt = test_one_epoch(\n",
    "            device, model, test_loader\n",
    "        )\n",
    "\n",
    "        # Save Best Model\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            snap = {\n",
    "                # state_dict: https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"min_loss\": best_test_loss,\n",
    "                \"optimizer\": optimizer.state_dict,\n",
    "            }\n",
    "            torch.save(snap, f\"checkpoints/{model_name}/models/best_model_snap.t7\")\n",
    "            torch.save(\n",
    "                model.state_dict, f\"checkpoints/{model_name}/models/best_model.t7\"\n",
    "            )\n",
    "\n",
    "            # Make confusion matrix figure\n",
    "            make_confusion_matrix(conf_mat, categories=classes)\n",
    "\n",
    "            # Save best model confusion matrix\n",
    "            delete_files(\n",
    "                f\"checkpoints/{model_name}/confusion_matrix/best\", \"*.png\"\n",
    "            )  # delete previous\n",
    "            plt.savefig(\n",
    "                f\"checkpoints/{model_name}/confusion_matrix/best/confusion_matrix_epoch{epoch+1}.png\",\n",
    "                bbox_inches=\"tight\",\n",
    "                dpi=300,\n",
    "            )  # save png\n",
    "            plt.close()\n",
    "\n",
    "            # Create classification report figure\n",
    "            sns.heatmap(pd.DataFrame(cls_rpt).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "\n",
    "            # save best model classification report\n",
    "            delete_files(\n",
    "                f\"checkpoints/{model_name}/classification_report/best\", \"*.png\"\n",
    "            )  # delete previous\n",
    "            plt.savefig(\n",
    "                f\"checkpoints/{model_name}/classification_report/best/classification_report_epoch{epoch+1}.png\",\n",
    "                bbox_inches=\"tight\",\n",
    "                dpi=300,\n",
    "            )  # save png\n",
    "\n",
    "        # Create confusion matrix figure\n",
    "        cm = make_confusion_matrix(conf_mat, categories=classes)\n",
    "        # plt.savefig(\n",
    "        #     f\"checkpoints/{model_name}/confusion_matrix/all/confusion_matrix_epoch{epoch+1}.png\",\n",
    "        #     bbox_inches=\"tight\",\n",
    "        #     dpi=300,\n",
    "        # )  # save .png\n",
    "        # plt.close()\n",
    "\n",
    "        # Create classification report figure\n",
    "        cr = sns.heatmap(pd.DataFrame(cls_rpt).iloc[:-1, :].T, annot=True, cmap=\"Blues\")\n",
    "        # plt.savefig(\n",
    "        #     f\"checkpoints/{model_name}/classification_report/all/classification_report_epoch{epoch+1}.png\",\n",
    "        #     bbox_inches=\"tight\",\n",
    "        #     dpi=300,\n",
    "        # )  # save .png\n",
    "        # plt.close()\n",
    "\n",
    "        # Save model\n",
    "        torch.save(snap, f\"checkpoints/{model_name}/models/model_snap.t7\")\n",
    "        torch.save(model.state_dict, f\"checkpoints/{model_name}/models/model.t7\")\n",
    "\n",
    "        boardio.add_scalar(\"Train Loss\", train_loss, epoch + 1)\n",
    "        boardio.add_scalar(\"Test Loss\", test_loss, epoch + 1)\n",
    "        boardio.add_scalar(\"Best Test Loss\", best_test_loss, epoch + 1)\n",
    "        boardio.add_scalar(\"Train Accuracy\", train_accuracy, epoch + 1)\n",
    "        boardio.add_scalar(\"Test Accuracy\", test_accuracy, epoch + 1)\n",
    "        boardio.add_scalars(\n",
    "            \"Loss\", {\"Training Loss\": train_loss, \"Test Loss\": test_loss}, epoch + 1\n",
    "        )\n",
    "        boardio.add_figure(\n",
    "            \"matplotlib\", cm\n",
    "        )\n",
    "\n",
    "        textio.cprint(\n",
    "            \"EPOCH:: %d, Training Loss: %f, Testing Loss: %f, Best Loss: %f\"\n",
    "            % (epoch + 1, train_loss, test_loss, best_test_loss)\n",
    "        )\n",
    "        textio.cprint(\n",
    "            \"EPOCH:: %d, Training Accuracy: %f Testing Accuracy: %f\"\n",
    "            % (epoch + 1, train_accuracy, test_accuracy)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702945fb-aa4b-4957-9e27-417dd00580b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    boardio = SummaryWriter(log_dir=\"checkpoints/\" + model_name)\n",
    "    _init_(model_name)\n",
    "\n",
    "    textio = IOStream(\"checkpoints/\" + model_name + \"/run.log\")\n",
    "    textio.cprint(model_name)\n",
    "\n",
    "    # Get training and test datasets\n",
    "    # max_points 1024, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 9216, 10240,\n",
    "    # 11264, 12288, 13312, 14336, 15360, 16384, 17408, 18432, 19456, 20480\n",
    "    trainset = PointCloudsInFiles(\n",
    "        train_dataset_path,\n",
    "        \"*.laz\",\n",
    "        \"Class\",\n",
    "        max_points=max_points,\n",
    "        use_columns=use_columns,\n",
    "    )\n",
    "    testset = PointCloudsInFiles(\n",
    "        test_dataset_path,\n",
    "        \"*.laz\",\n",
    "        \"Class\",\n",
    "        max_points=max_points,\n",
    "        use_columns=use_columns,\n",
    "    )\n",
    "\n",
    "    # Load training and test datasets\n",
    "    train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Define device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = PointCNN(numfeatures=len(use_columns), numclasses=len(classes))\n",
    "\n",
    "    checkpoint = None\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    train(\n",
    "        device=device,\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        boardio=boardio,\n",
    "        textio=textio,\n",
    "        checkpoint=checkpoint,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4421464-955b-4ddd-a0df-5e913e42b4f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T20:37:46.467144Z",
     "iopub.status.busy": "2022-06-09T20:37:46.467144Z",
     "iopub.status.idle": "2022-06-09T20:37:46.477100Z",
     "shell.execute_reply": "2022-06-09T20:37:46.476104Z",
     "shell.execute_reply.started": "2022-06-09T20:37:46.467144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b503904-5fa3-444f-a1c8-04f1c5418461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-09T20:37:50.129057Z",
     "iopub.status.busy": "2022-06-09T20:37:50.128094Z",
     "iopub.status.idle": "2022-06-09T20:37:50.148969Z",
     "shell.execute_reply": "2022-06-09T20:37:50.147973Z",
     "shell.execute_reply.started": "2022-06-09T20:37:50.129057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864951b-74fe-43e8-b5b1-b66549a63ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch Python 3.9",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
